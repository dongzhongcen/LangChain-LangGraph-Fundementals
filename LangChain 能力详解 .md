# LangChain èƒ½åŠ›è¯¦è§£

> æœ¬æ–‡ä¸ºä¸ªäººå­¦ä¹ ç¬”è®°ï¼ŒåŸºäºè¯¾ç¨‹èµ„æ–™æ•´ç†ï¼Œä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œå¤ä¹ ã€‚

## ğŸ“‘ ç›®å½•
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹)
- [LangChainæ ¸å¿ƒæ¦‚å¿µ](#langchainæ ¸å¿ƒæ¦‚å¿µ)
- [èŠå¤©æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›](#èŠå¤©æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›)
- [æ¶ˆæ¯æœºåˆ¶](#æ¶ˆæ¯æœºåˆ¶)
- [æç¤ºè¯æ¨¡æ¿](#æç¤ºè¯æ¨¡æ¿)
- [å°‘æ ·æœ¬æç¤º](#å°‘æ ·æœ¬æç¤º)
- [è¾“å‡ºè§£æå™¨](#è¾“å‡ºè§£æå™¨)
- [æ–‡æ¡£åŠ è½½å™¨](#æ–‡æ¡£åŠ è½½å™¨)
- [æ–‡æœ¬åˆ†å‰²å™¨](#æ–‡æœ¬åˆ†å‰²å™¨)
- [æ–‡æœ¬å‘é‡ä¸å‘é‡å­˜å‚¨](#æ–‡æœ¬å‘é‡ä¸å‘é‡å­˜å‚¨)
- [æ£€ç´¢å™¨](#æ£€ç´¢å™¨)
- [RAGå®æˆ˜æ¡ˆä¾‹](#ragå®æˆ˜æ¡ˆä¾‹)

---

## ç¯å¢ƒé…ç½®

### Pythonç¯å¢ƒ
- Python 3.13

### æ‰€éœ€ä¾èµ–åŒ…
```python
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒä¾èµ–
langchain-openai==0.3.33
langchain==0.3.27
langchain-deepseek==0.1.4
langchain-ollama==0.3.6
langchain-tavily==0.2.12
langchain-chroma==0.2.5
langchain-community==0.3.22
nltk==3.9.2
langchain-redis==0.2.4
unstructured==0.18.15
markdown==3.9
redisvl==0.10.0

# ç¬¬äºŒéƒ¨åˆ†ï¼šPineconeï¼ˆæ³¨æ„å®‰è£…é¡ºåºé—®é¢˜ï¼‰
pinecone==7.3.0
langchain-pinecone==0.2.12
```

**âš ï¸ æ³¨æ„äº‹é¡¹**ï¼š
- Pineconeç›¸å…³åŒ…éœ€è¦åœ¨å­¦ä¹ åˆ°å‘é‡å­˜å‚¨éƒ¨åˆ†å†å®‰è£…
- å®‰è£…langchain-pineconeä¼šå½±å“Redisçš„MMRæœç´¢åŠŸèƒ½
- é«˜ç‰ˆæœ¬Pineconeå¯èƒ½å­˜åœ¨numpyå…¼å®¹æ€§é—®é¢˜

å®‰è£…å‘½ä»¤ï¼š
```bash
pip install -r requirements.txt
```

---

## å¿«é€Ÿä¸Šæ‰‹

### ä¸ºä»€ä¹ˆè¦ç”¨LangChainï¼Ÿ

åŸç”ŸLLMå¼€å‘é¢ä¸´çš„é—®é¢˜ï¼š
- æç¤ºè¯ä¸è§„èŒƒï¼Œç»“æœå®¹æ˜“å‡ºç°å¹»è§‰
- æ¨¡å‹åˆ‡æ¢å›°éš¾
- è¾“å‡ºéç»“æ„åŒ–ï¼Œéš¾ä»¥ä¸ç¨‹åºæ¥å£å¯¹æ¥
- çŸ¥è¯†é™ˆæ—§ï¼Œæ— æ³•è·å–å®æ—¶ä¿¡æ¯
- éš¾ä»¥è¿æ¥å¤–éƒ¨å·¥å…·å’Œç³»ç»Ÿ

LangChainçš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯è§£å†³è¿™äº›é—®é¢˜ï¼Œå°†NLPæµç¨‹æ‹†è§£ä¸ºæ ‡å‡†åŒ–ç»„ä»¶ã€‚

### æœ€ç®€å•çš„å¯¹è¯ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

# 1. å®šä¹‰å¤§æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini")

# 2. å®šä¹‰æ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="Translate the following from English into Chinese"),
    HumanMessage(content="hi!")
]

# 3. å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4. å®šä¹‰é“¾
chain = model | parser

# 5. æ‰§è¡Œé“¾
result = chain.invoke(messages)
print(result)  # è¾“å‡ºï¼šä½ å¥½ï¼
```

### å…³é”®æ¦‚å¿µå¼•å‡º

**Runnableæ¥å£**ï¼šLangChainç»„ä»¶çš„åŸºç¡€æ¥å£ï¼Œæä¾›ï¼š
- **Invoked**ï¼šå•ä¸ªè¾“å…¥è½¬æ¢ä¸ºè¾“å‡º
- **Batched**ï¼šæ‰¹é‡å¤„ç†
- **Streamed**ï¼šæµå¼ä¼ è¾“
- **Inspected**ï¼šæ£€æŸ¥åŠŸèƒ½
- **Composed**ï¼šç»„åˆèƒ½åŠ›

**LCEL (LangChain Expression Language)**ï¼šå£°æ˜å¼ç¼–ç¨‹æ–¹å¼ï¼Œé€šè¿‡`|`æ“ä½œç¬¦åˆå¹¶Runnableå¯¹è±¡ã€‚

```python
# ä»¥ä¸‹ä¸‰ç§å†™æ³•ç­‰ä»·
chain = model | parser
chain = RunnableSequence(first=model, last=parser)
chain = model.pipe(parser)
```

---

## èŠå¤©æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›

### 1. å®šä¹‰èŠå¤©æ¨¡å‹

#### æ–¹å¼1ï¼šChatOpenAIï¼ˆæ˜ç¡®æŒ‡å®šï¼‰

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,           # æ§åˆ¶éšæœºæ€§ï¼Œ0æœ€ä¿å®ˆ
    max_tokens=None,         # æœ€å¤§ç”Ÿæˆtokenæ•°
    timeout=None,            # è¶…æ—¶æ—¶é—´
    max_retries=2,           # æœ€å¤§é‡è¯•æ¬¡æ•°
    # api_key="your-key",    # å¯ä»ç¯å¢ƒå˜é‡è¯»å–
    # base_url="...",        # APIè¯·æ±‚åŸºç¡€URL
)
```

#### æ–¹å¼2ï¼šinit_chat_modelï¼ˆå·¥å‚å‡½æ•°ï¼‰

```python
from langchain.chat_models import init_chat_model

# åŸºæœ¬ç”¨æ³•
gpt_model = init_chat_model("gpt-4o-mini", model_provider="openai")
deepseek_model = init_chat_model("deepseek-chat", model_provider="deepseek")

# å¯é…ç½®æ¨¡å‹
configurable_model = init_chat_model(
    model="gpt-4o-mini",
    temperature=0,
    configurable_fields=("model", "temperature"),
    config_prefix="first"
)

# åŠ¨æ€é…ç½®
result = configurable_model.invoke(
    "what's your name",
    config={
        "configurable": {
            "first_model": "deepseek-chat",
            "first_temperature": 0.5
        }
    }
)
```

#### æ–¹å¼3ï¼šæœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼ˆChatOllamaï¼‰

```python
from langchain_ollama import ChatOllama

ollama_model = ChatOllama(
    model="deepseek-r1:70b",
    base_url='http://192.168.100.220:11434',
    num_ctx=2048,    # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    num_gpu=1        # GPUæ•°é‡
)
```

### 2. å·¥å…·è°ƒç”¨

å·¥å…·è°ƒç”¨è®©LLMèƒ½å¤Ÿä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’ï¼Œæ‰©å±•èƒ½åŠ›è¾¹ç•Œã€‚

#### åˆ›å»ºå·¥å…·çš„å¤šç§æ–¹å¼

**æ–¹å¼1ï¼š@toolè£…é¥°å™¨ï¼ˆæœ€å¸¸ç”¨ï¼‰**

```python
from langchain_core.tools import tool
from typing_extensions import Annotated

# éœ€è¦æ–‡æ¡£å­—ç¬¦ä¸²
@tool
def multiply(a: int, b: int) -> int:
    """Multiply two integers."""
    return a * b

# ä½¿ç”¨Annotatedæä¾›å‚æ•°æè¿°
@tool
def add(
    a: Annotated[int, ..., "First integer"],
    b: Annotated[int, ..., "Second integer"]
) -> int:
    """Add two integers."""
    return a + b
```

**æ–¹å¼2ï¼šä¾èµ–Pydanticç±»**

```python
from pydantic import BaseModel, Field

class AddInput(BaseModel):
    """Add two integers."""
    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")

@tool(args_schema=AddInput)
def add(a: int, b: int) -> int:
    return a + b  # æ— éœ€æ–‡æ¡£å­—ç¬¦ä¸²
```

**æ–¹å¼3ï¼šStructuredTool.from_function**

```python
from langchain_core.tools import StructuredTool

def multiply(a: int, b: int) -> int:
    return a * b

calculator_tool = StructuredTool.from_function(
    func=multiply,
    name="Calculator",
    description="ä¸¤æ•°ç›¸ä¹˜",
    args_schema=CalculatorInput,
    response_format="content_and_artifact"  # è¿”å›å†…å®¹å’ŒåŸå§‹æ•°æ®
)
```

#### ç»‘å®šå·¥å…·

```python
# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
tools = [add, multiply]
model_with_tools = model.bind_tools(tools)

# æˆ–å¼ºåˆ¶è°ƒç”¨å·¥å…·
model_with_tools = model.bind_tools(tools, tool_choice="any")
```

#### å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹

```python
from langchain_core.messages import HumanMessage

# 1. å®šä¹‰æ¶ˆæ¯
messages = [HumanMessage("9ä¹˜6ç­‰äºå¤šå°‘ï¼Ÿ5åŠ 3ç­‰äºå¤šå°‘ï¼Ÿ")]

# 2. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè·å–å·¥å…·è°ƒç”¨æŒ‡ä»¤
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# 3. æ‰§è¡Œå·¥å…·è°ƒç”¨
for tool_call in ai_msg.tool_calls:
    selected_tool = {"add": add, "multiply": multiply}[tool_call["name"].lower()]
    tool_msg = selected_tool.invoke(tool_call)
    messages.append(tool_msg)

# 4. ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šè·å–æœ€ç»ˆç­”æ¡ˆ
result = model.invoke(messages)
print(result.content)  # è¾“å‡ºï¼š9ä¹˜6ç­‰äº54ï¼Œ5åŠ 3ç­‰äº8ã€‚
```

#### LangChainå†…ç½®å·¥å…·ç¤ºä¾‹ï¼šTavilyæœç´¢

```python
from langchain_tavily import TavilySearch

# é…ç½®ç¯å¢ƒå˜é‡ TAVILY_API_KEY
tool = TavilySearch(max_results=4)
model_with_tools = model.bind_tools([tool])

messages = [HumanMessage("ä¸­å›½è¥¿å®‰ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

for tool_call in ai_msg.tool_calls:
    tool_msg = tool.invoke(tool_call)
    messages.append(tool_msg)

result = model_with_tools.invoke(messages)
print(result.content)
```

### 3. ç»“æ„åŒ–è¾“å‡º

å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼ï¼ˆJSONã€Pydanticå¯¹è±¡ç­‰ï¼‰ã€‚

```python
from pydantic import BaseModel, Field
from typing import Optional

class Joke(BaseModel):
    """ç»™ç”¨æˆ·è®²ä¸€ä¸ªç¬‘è¯ã€‚"""
    setup: str = Field(description="è¿™ä¸ªç¬‘è¯çš„å¼€å¤´")
    punchline: str = Field(description="è¿™ä¸ªç¬‘è¯çš„å¦™è¯­")
    rating: Optional[int] = Field(default=None, description="è¯„åˆ†1-10")

# ç»‘å®šç»“æ„åŒ–è¾“å‡º
structured_model = model.with_structured_output(Joke)
result = structured_model.invoke("ç»™æˆ‘è®²ä¸€ä¸ªå…³äºå”±æ­Œçš„ç¬‘è¯")
print(result)  # è¾“å‡ºPydanticå¯¹è±¡
```

**åµŒå¥—ç»“æ„ç¤ºä¾‹**ï¼š

```python
class Data(BaseModel):
    """è·å–å…³äºç¬‘è¯çš„æ•°æ®ã€‚"""
    jokes: List[Joke]

structured_model = model.with_structured_output(Data)
result = structured_model.invoke("åˆ†åˆ«è®²ä¸€ä¸ªå…³äºå”±æ­Œå’Œè·³èˆçš„ç¬‘è¯")
```

### 4. æµå¼ä¼ è¾“

```python
# åŒæ­¥æµå¼
chunks = []
for chunk in model.stream("è®²ä¸€ä¸ª50å­—ç¬‘è¯"):
    chunks.append(chunk)
    print(chunk.content, end="|", flush=True)

# å¼‚æ­¥æµå¼
import asyncio

async def async_stream():
    async for chunk in model.astream("è®²ä¸€ä¸ª50å­—ç¬‘è¯"):
        print(chunk.content, end="", flush=True)

asyncio.run(async_stream())

# å¸¦è§£æå™¨çš„æµå¼
chain = model | StrOutputParser()
for chunk in chain.stream("å†™ä¸€æ®µå…³äºçˆ±æƒ…çš„æ­Œè¯"):
    print(chunk, end="|", flush=True)
```

#### è‡ªå®šä¹‰æµå¼è§£æå™¨

```python
from typing import Iterator

def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:
    buffer = ""
    for chunk in input:
        buffer += chunk
        while "ã€‚" in buffer:
            stop_index = buffer.index("ã€‚")
            yield [buffer[:stop_index].strip()]
            buffer = buffer[stop_index + 1:]
    yield [buffer.strip()]

chain = model | StrOutputParser() | split_into_list
```

### 5. ä½¿ç”¨LangSmithè¿½è¸ª

```python
# é…ç½®ç¯å¢ƒå˜é‡
# LANGSMITH_TRACING="true"
# LANGSMITH_API_KEY="ä½ çš„LangSmith API Key"

# ä»»æ„ä»£ç æ‰§è¡Œåï¼Œåœ¨LangSmithå¹³å°æŸ¥çœ‹è¿½è¸ªä¿¡æ¯
```

---

## æ¶ˆæ¯æœºåˆ¶

### æ¶ˆæ¯ç±»å‹

| ç±»å‹ | å¯¹åº”è§’è‰² | æè¿° |
|------|----------|------|
| SystemMessage | system | ç³»ç»ŸæŒ‡ä»¤ï¼Œè®¾å®šå¯¹è¯åŸºè°ƒ |
| HumanMessage | user | ç”¨æˆ·è¾“å…¥ |
| AIMessage | assistant | æ¨¡å‹å“åº” |
| ToolMessage | tool | å·¥å…·è°ƒç”¨ç»“æœ |

### å¤šè½®å¯¹è¯ä¸å†…å­˜

```python
from langchain_core.messages import HumanMessage, AIMessage

# æ‰‹åŠ¨ç»´æŠ¤å†å²æ¶ˆæ¯
messages = [
    HumanMessage(content="Hi! I'm Bob"),
    AIMessage(content="Hello Bob! How can I assist you today?"),
    HumanMessage(content="What's my name?"),
]
model.invoke(messages).pretty_print()
```

### æ¶ˆæ¯è£å‰ª

```python
from langchain_core.messages import trim_messages

trimmer = trim_messages(
    max_tokens=65,           # æœ€å¤§tokenæ•°
    strategy="last",         # ä¿ç•™æœ€åçš„æ¶ˆæ¯
    token_counter=model,     # tokenè®¡æ•°æ–¹æ³•
    include_system=True,     # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    allow_partial=False,     # ä¸å…è®¸æ‹†åˆ†æ¶ˆæ¯
    start_on="human",        # ç¡®ä¿ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯human
)

chain = trimmer | model
```

### æ¶ˆæ¯è¿‡æ»¤

```python
from langchain_core.messages import filter_messages

# æŒ‰ç±»å‹è¿‡æ»¤
filter_messages(messages, include_types="human")

# æŒ‰ç±»å‹+IDè¿‡æ»¤
filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=["3"])
```

### æ¶ˆæ¯åˆå¹¶

```python
from langchain_core.messages import merge_message_runs

merged = merge_message_runs(messages)
chain = merge_message_runs() | model
```

---

## æç¤ºè¯æ¨¡æ¿

### å­—ç¬¦ä¸²æ¨¡æ¿

```python
from langchain_core.prompts import PromptTemplate

# æ–¹å¼1ï¼šfrom_template
prompt_template = PromptTemplate.from_template("Translate the following into {language}")

# æ–¹å¼2ï¼šç›´æ¥åˆå§‹åŒ–
prompt_template = PromptTemplate(
    input_variables=["language"],
    template="Translate the following into {language}",
)

result = prompt_template.invoke({"language": "Chinese"})
```

### èŠå¤©æ¶ˆæ¯æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "Translate the following into {language}."),
    ("user", "{text}")
])

# å®ä¾‹åŒ–
messages = prompt_template.invoke({
    "language": "Chinese",
    "text": "what is your name?"
}).to_messages()

# é“¾å¼è°ƒç”¨
chain = prompt_template | model | StrOutputParser()
```

### æ¶ˆæ¯å ä½ç¬¦

```python
from langchain_core.prompts import MessagesPlaceholder

prompt_template = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹"),
    MessagesPlaceholder("msgs")  # æˆ– ("placeholder", "{msgs}")
])

messages_to_pass = [
    HumanMessage(content="ä¸­å›½é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"),
    AIMessage(content="ä¸­å›½é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"),
    HumanMessage(content="é‚£æ³•å›½å‘¢ï¼Ÿ")
]

result = prompt_template.invoke({"msgs": messages_to_pass})
```

### ä½¿ç”¨LangChain Hub

```python
from langsmith import Client

client = Client()
prompt = client.pull_prompt("hardkothari/prompt-maker", include_model=True)
chain = prompt | model

while True:
    task = input("ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ")
    lazy_prompt = input("ä½ å½“å‰çš„æç¤ºæ˜¯ä»€ä¹ˆï¼Ÿ")
    chain.invoke({'lazy_prompt': lazy_prompt, 'task': task}).pretty_print()
```

---

## å°‘æ ·æœ¬æç¤º

### åŸºæœ¬ç”¨æ³•

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

# å®šä¹‰ç¤ºä¾‹
examples = [
    {"input": "2 2", "output": "4"},
    {"input": "2 3", "output": "5"},
]

# å®šä¹‰ç¤ºä¾‹æ¨¡æ¿
example_prompt = ChatPromptTemplate([
    ("human", "{input}"),
    ("ai", "{output}"),
])

# åˆ›å»ºå°‘æ ·æœ¬æç¤º
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# ç»„åˆæœ€ç»ˆæç¤º
final_prompt = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªç¥å¥‡çš„æ•°å­¦å¥‡æ‰ã€‚"),
    few_shot_prompt,
    ("human", "{input}"),
])

chain = final_prompt | model
chain.invoke({"input": "What is 2 9?"}).pretty_print()  # è¾“å‡ºï¼š11
```

### æ¨ç†å¼•å¯¼ç¤ºä¾‹

```python
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# å­—ç¬¦ä¸²æ¨¡æ¿
example_prompt = PromptTemplate.from_template("Question: {question}\n{answer}")

# ç¤ºä¾‹é›†ï¼ˆåŒ…å«æ¨ç†è¿‡ç¨‹ï¼‰
examples = [
    {
        "question": "æç™½å’Œæœç”«ï¼Œè°æ›´é•¿å¯¿ï¼Ÿ",
        "answer": """æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šæç™½äº«å¹´å¤šå°‘å²ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šæç™½äº«å¹´61å²ã€‚
åç»­é—®é¢˜ï¼šæœç”«äº«å¹´å¤šå°‘å²ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šæœç”«äº«å¹´58å²ã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šæç™½"""
    },
    # ... æ›´å¤šç¤ºä¾‹
]

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

prompt_messages = prompt.invoke({
    "input": "ã€Šæ•™çˆ¶ã€‹å’Œã€Šæ˜Ÿçƒå¤§æˆ˜ã€‹çš„å¯¼æ¼”æ¥è‡ªåŒä¸€ä¸ªå›½å®¶å—ï¼Ÿ"
}).to_messages()
```

### ç¤ºä¾‹é€‰æ‹©å™¨

#### æŒ‰é•¿åº¦é€‰æ‹©

```python
from langchain_core.example_selectors import LengthBasedExampleSelector

example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    max_length=25,
)

dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```

#### æŒ‰è¯­ä¹‰ç›¸ä¼¼æ€§é€‰æ‹©

```python
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    Chroma,
    k=1,
)
```

#### æŒ‰MMRé€‰æ‹©ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰

```python
from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector

example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    Chroma,
    k=2,
)
```

---

## è¾“å‡ºè§£æå™¨

### æ–‡æœ¬è§£æå™¨

```python
from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()
chain = model | parser
```

### ç»“æ„åŒ–å¯¹è±¡è§£æå™¨

```python
from langchain_core.output_parsers import PydanticOutputParser

parser = PydanticOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser
```

### JSONè§£æå™¨

```python
from langchain_core.output_parsers import JsonOutputParser

parser = JsonOutputParser()
# æˆ–å¸¦PydanticéªŒè¯
parser = JsonOutputParser(pydantic_object=Joke)
```

å…¶ä»–è§£æå™¨ï¼šXMLOutputParserã€YamlOutputParserã€CommaSeparatedListOutputParserã€EnumOutputParserç­‰ã€‚

---

## æ–‡æ¡£åŠ è½½å™¨

### Documentå¯¹è±¡

```python
from langchain_core.documents import Document

documents = [
    Document(
        page_content="ç‹—æ˜¯å¾ˆå¥½çš„ä¼´ä¾£ï¼Œä»¥å¿ è¯šå’Œå‹å¥½è€Œé—»åã€‚",
        metadata={"source": "mammal-pets-doc"},
    ),
    # ...
]
```

### åŠ è½½PDF

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("./Docs/PDF/æ–‡æ¡£.pdf")
docs = loader.load()  # æ¯é¡µä¸€ä¸ªDocument

print(f"æ€»é¡µæ•°ï¼š{len(docs)}")
print(f"ç¬¬ä¸€é¡µå†…å®¹ï¼š{docs[0].page_content[:200]}")
print(f"ç¬¬ä¸€é¡µå…ƒæ•°æ®ï¼š{docs[0].metadata}")
```

### åŠ è½½Markdown

```python
from langchain_community.document_loaders import UnstructuredMarkdownLoader

# singleæ¨¡å¼ï¼šæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªDocument
loader = UnstructuredMarkdownLoader("./Docs/Markdown/æ–‡æ¡£.md", mode="single")
data = loader.load()

# elementsæ¨¡å¼ï¼šæ‹†åˆ†ä¸ºå¤šä¸ªå…ƒç´ 
loader = UnstructuredMarkdownLoader("./Docs/Markdown/æ–‡æ¡£.md", mode="elements")
data = loader.load()  # åŒ…å«Titleã€ListItemã€NarrativeTextç­‰ç±»å‹
```

---

## æ–‡æœ¬åˆ†å‰²å™¨

### åŸºäºå­—ç¬¦é•¿åº¦æ‹†åˆ†

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n\n",        # åˆ†éš”ç¬¦
    chunk_size=100,          # ç›®æ ‡å—å¤§å°
    chunk_overlap=20,        # å—é‡å å¤§å°
    length_function=len,     # é•¿åº¦è®¡ç®—å‡½æ•°
    is_separator_regex=False,
)

texts = text_splitter.split_documents(documents)
```

### åŸºäºTokené•¿åº¦æ‹†åˆ†

```python
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",  # GPT-4ã€GPT-3.5-turboçš„ç¼–ç æ–¹å¼
    chunk_size=200,
    chunk_overlap=50,
)
```

### ç¡¬çº¦æŸé•¿åº¦æ‹†åˆ†

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    chunk_size=100,
    chunk_overlap=0,
)
```

### ä»£ç æ–‡æ¡£æ‹†åˆ†

```python
from langchain_text_splitters import PythonCodeTextSplitter

python_splitter = PythonCodeTextSplitter(chunk_size=50, chunk_overlap=0)
python_docs = python_splitter.create_documents([PYTHON_CODE])
```

---

## æ–‡æœ¬å‘é‡ä¸å‘é‡å­˜å‚¨

### åµŒå…¥æ¨¡å‹

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# åµŒå…¥æ–‡æ¡£åˆ—è¡¨
texts = [doc.page_content for doc in documents]
documents_vector = embeddings.embed_documents(texts)
print(f"å‘é‡ç»´åº¦ï¼š{len(documents_vector[0])}")  # 3072ç»´

# åµŒå…¥å•ä¸ªæŸ¥è¯¢
query_vector = embeddings.embed_query("é¡¹ç›®ä¸­é‡åˆ°äº†å“ªäº›æŒ‘æˆ˜ï¼Ÿ")
```

### å†…å­˜å‘é‡å­˜å‚¨

```python
from langchain_core.vectorstores import InMemoryVectorStore

# åˆå§‹åŒ–
vector_store = InMemoryVectorStore(embedding=embeddings)

# æ·»åŠ æ–‡æ¡£
ids = vector_store.add_documents(documents=documents)

# è·å–æ–‡æ¡£
docs = vector_store.get_by_ids(ids[:3])

# ç›¸ä¼¼æ€§æœç´¢
search_docs = vector_store.similarity_search(query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ", k=2)

# å…ƒæ•°æ®è¿‡æ»¤
def filter_function(doc: Document) -> bool:
    return doc.metadata.get("source") == "æœŸæœ›çš„source"

search_docs = vector_store.similarity_search(
    query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ",
    k=2,
    filter=filter_function
)

# åˆ é™¤æ–‡æ¡£
vector_store.delete(ids=ids[:3])
```

### Rediså‘é‡å­˜å‚¨

```python
from langchain_redis import RedisConfig, RedisVectorStore
from redisvl.query.filter import Tag, Num

# é…ç½®
config = RedisConfig(
    index_name="qa",
    redis_url="redis://localhost:6379",
    metadata_schema=[
        {"name": "category", "type": "tag"},
        {"name": "num", "type": "numeric"},
    ],
)

# åˆå§‹åŒ–
vector_store = RedisVectorStore(embeddings, config=config)

# æ·»åŠ æ–‡æ¡£æ—¶æ·»åŠ å…ƒæ•°æ®
for i, doc in enumerate(documents, start=1):
    doc.metadata["category"] = "QA"
    doc.metadata["num"] = i

ids = vector_store.add_documents(documents=documents)

# ç›¸ä¼¼æ€§æœç´¢å¸¦åˆ†æ•°
scored_results = vector_store.similarity_search_with_score(
    query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ",
    k=4
)

# å…ƒæ•°æ®è¿‡æ»¤
filter_condition = (Tag("category") == "qa") & (Num("num") < 50)
scored_results = vector_store.similarity_search_with_score(
    query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ",
    k=2,
    filter=filter_condition
)

# MMRæœç´¢
mmr_results = vector_store.max_marginal_relevance_search(
    query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ",
    k=2,
    fetch_k=10,
    filter=filter_condition
)
```

### Pineconeå‘é‡å­˜å‚¨

```python
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore

# åˆå§‹åŒ–Pinecone
pc = Pinecone()
index_name = "qa"

# åˆ›å»ºç´¢å¼•
if not pc.has_index(index_name):
    pc.create_index(
        name=index_name,
        dimension=3072,
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# è·å–ç´¢å¼•
index = pc.Index(index_name)

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vector_store = PineconeVectorStore(embedding=embeddings, index=index)

# æ·»åŠ æ–‡æ¡£
ids = vector_store.add_documents(documents=documents)

# ç›¸ä¼¼æ€§æœç´¢
search_docs = vector_store.similarity_search(
    query="æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ",
    k=2,
    filter={"category": "QA"}
)

# åˆ é™¤
vector_store.delete(delete_all=True)  # å…¨é‡åˆ é™¤
vector_store.delete(ids=delete_ids)    # æŒ‡å®šIDåˆ é™¤
```

---

## æ£€ç´¢å™¨

### ä»å‘é‡å­˜å‚¨åˆ›å»ºæ£€ç´¢å™¨

```python
# åŸºæœ¬æ£€ç´¢å™¨
retriever = vector_store.as_retriever()
docs = retriever.invoke("æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ")

# é…ç½®å‚æ•°
retriever = vector_store.as_retriever(
    search_type="similarity",  # æˆ– "mmr", "similarity_score_threshold"
    search_kwargs={"k": 2}
)

# MMRé…ç½®
retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 2,
        "fetch_k": 10
    }
)
```

### è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
from langchain_core.runnables import chain
from typing import List
from langchain_core.documents import Document

@chain
def custom_retriever(query: str) -> List[Document]:
    return vector_store.similarity_search(query, k=2)

docs = custom_retriever.invoke("æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ")
```

---

## RAGå®æˆ˜æ¡ˆä¾‹

### å®Œæ•´RAGæµç¨‹

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_redis import RedisConfig, RedisVectorStore
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# 1. åˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini")
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# 2. é…ç½®å‘é‡å­˜å‚¨
config = RedisConfig(
    index_name="qa",
    redis_url="redis://192.168.100.238:6379",
    metadata_schema=[
        {"name": "category", "type": "tag"},
        {"name": "num", "type": "numeric"},
    ],
)
vector_store = RedisVectorStore(embeddings, config=config)
retriever = vector_store.as_retriever()

# 3. å®šä¹‰æç¤ºè¯æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("human", """ä½ æ˜¯è´Ÿè´£å›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æœ€å¤šåªç”¨ä¸‰å¥è¯ï¼Œå›ç­”è¦ç®€æ˜æ‰¼è¦ã€‚

Question: {question}
Context: {context}
Answer:""")
])

# 4. æ–‡æ¡£æ ¼å¼åŒ–å‡½æ•°
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 5. æ„å»ºRAGé“¾
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | StrOutputParser()
)

# 6. æµå¼æ‰§è¡Œ
for chunk in rag_chain.stream("æ•°æ®åº“è¡¨æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ"):
    print(chunk, end="", flush=True)
```

### äº¤äº’å¼RAGé—®ç­”

```python
while True:
    question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰ï¼š").strip()
    if question.lower() in ["é€€å‡º", "quit"]:
        break
    if not question:
        continue
    
    print("å›ç­”ï¼š", end="", flush=True)
    for chunk in rag_chain.stream(question):
        print(chunk, end="", flush=True)
    print()
```

---

## ğŸ“Œ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

| ç»„ä»¶ | ä½œç”¨ | å¸¸ç”¨ç±»/æ–¹æ³• |
|------|------|------------|
| **èŠå¤©æ¨¡å‹** | ä¸LLMäº¤äº’ | ChatOpenAI, init_chat_model |
| **æ¶ˆæ¯** | é€šä¿¡å•ä½ | SystemMessage, HumanMessage, AIMessage |
| **æç¤ºè¯æ¨¡æ¿** | åŠ¨æ€ç”Ÿæˆæç¤ºè¯ | PromptTemplate, ChatPromptTemplate |
| **è¾“å‡ºè§£æå™¨** | ç»“æ„åŒ–è¾“å‡º | StrOutputParser, PydanticOutputParser |
| **å·¥å…·è°ƒç”¨** | æ‰©å±•LLMèƒ½åŠ› | @tool, bind_tools() |
| **æ–‡æ¡£åŠ è½½å™¨** | åŠ è½½å„ç±»æ–‡æ¡£ | PyPDFLoader, UnstructuredMarkdownLoader |
| **æ–‡æœ¬åˆ†å‰²å™¨** | åˆ‡åˆ†æ–‡æ¡£ | CharacterTextSplitter, RecursiveCharacterTextSplitter |
| **åµŒå…¥æ¨¡å‹** | æ–‡æœ¬è½¬å‘é‡ | OpenAIEmbeddings |
| **å‘é‡å­˜å‚¨** | å­˜å‚¨å’Œæ£€ç´¢å‘é‡ | InMemoryVectorStore, RedisVectorStore |
| **æ£€ç´¢å™¨** | ç»Ÿä¸€æ£€ç´¢æ¥å£ | as_retriever(), è‡ªå®šä¹‰@chain |
| **LCEL** | å£°æ˜å¼é“¾å¼ç¼–ç¨‹ | `\|` æ“ä½œç¬¦, RunnableSequence |

---

> **ä¸ªäººå­¦ä¹ å¿ƒå¾—**ï¼š
> - LangChainçš„æ ¸å¿ƒæ€æƒ³æ˜¯**ç»„ä»¶åŒ–**å’Œ**å¯ç»„åˆæ€§**ï¼Œæ¯ä¸ªç»„ä»¶éƒ½å®ç°Runnableæ¥å£
> - LCELè®©æ„å»ºå¤æ‚æµç¨‹å˜å¾—ç®€å•ç›´è§‚
> - RAGæ˜¯å½“å‰æœ€å®ç”¨çš„LLMåº”ç”¨æ¨¡å¼ï¼ŒæŒæ¡å¥½æ–‡æ¡£åŠ è½½â†’åˆ†å‰²â†’åµŒå…¥â†’å­˜å‚¨â†’æ£€ç´¢â†’ç”Ÿæˆçš„å®Œæ•´æµç¨‹
> - é‡åˆ°é—®é¢˜æ—¶ï¼ŒæŸ¥çœ‹å®˜æ–¹æ–‡æ¡£å’Œæºç æ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹å¼